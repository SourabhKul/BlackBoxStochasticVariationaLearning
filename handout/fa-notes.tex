\documentclass[11pt]{article}

\oddsidemargin=0in
\evensidemargin=0in
\textwidth=6.3in
\topmargin=-0.5in
\textheight=9in

\parindent=0in
%\pagestyle{empty}

\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{url}


\newcommand{\argmax}{\mathop{\arg\max}}
\newcommand{\deriv}[2]{\frac{\partial{#1}}{\partial {#2}} }
\newcommand{\dsep}{\mbox{dsep}}
\newcommand{\Pa}{\mathop{Pa}}
\newcommand{\ND}{\mbox{ND}}
\newcommand{\De}{\mbox{De}}
\newcommand{\Ch}{\mbox{Ch}}
\newcommand{\graphG}{{\mathcal{G}}}
\newcommand{\graphH}{{\mathcal{H}}}
\newcommand{\setA}{\mathcal{A}}
\newcommand{\setB}{\mathcal{B}}
\newcommand{\setS}{\mathcal{S}}
\newcommand{\setV}{\mathcal{V}}
\DeclareMathOperator*{\union}{\bigcup}
\DeclareMathOperator*{\intersection}{\bigcap}
\DeclareMathOperator*{\Val}{Val}
\newcommand{\mbf}[1]{{\mathbf{#1}}}
\newcommand{\eq}{\!=\!}
\newcommand{\cut}[1]{{}}

\begin{document}

%%%(change to appropriate class and semester)
{\centering
  \rule{6.3in}{2pt}
  \vspace{1em}
  \Large{Notes on Factor Analysis\\}
  Benjamin M. Marlin\\
  \vspace{0.1em}
  \rule{6.3in}{1.5pt}
}
\vspace{1pc}


\section{Introduction}


Factor analysis is a classical statistical model for linear manifolds based 
on the multivariate normal distribution. The model asserts that real-valued data 
$\mbf{x}\in\mathbb{R}^D$ are generated in a two stage
process that starts by first generating a low-dimensional latent factor 
vector $\mbf{z}\in\mathbb{R}^K$ from
a multivariate normal distribution. The observed $\mbf{x}$'s are then generated by a 
linear combination of basis vectors weighted
by the latent factor values: $\mbf{W}\mbf{z}$ with independent 
Gaussian noise added.
The standard probabilistic model for factor analysis is shown below: 

\begin{align}
	\label{eq:pz}
	P_{\theta}(\mbf{Z}=\mbf{z}) &= \mathcal{N}(\mbf{z}; 0,I)\\
	\label{eq:pxgz}
	P_{\theta}(\mbf{X}=\mbf{x}|\mbf{Z}=\mbf{z})&=\mathcal{N}(\mbf{x}; \mbf{Wz},\Psi)
\end{align}

$\mbf{W}$ is a $D\times K$ matrix or arbitrary real values. $\Psi$ is a $D\times D$ covariance
matrix that is restricted to be diagonal. The on-diagonal entries must all be positive,
and the off-diagonal entries are all zero. The parameters are thus $\theta=[\mbf{W},\Psi]$.

\section{Exact Joint, Marginal, and Conditional Distributions}

Since the data variables $\mbf{X}$ are conditionally normally distributed and the 
latent factors $\mbf{Z}$  are jointly multivariate normal, the joint
distribution of $\mbf{X}$ and $\mbf{Z}$ is also multivariate normal. The joint 
distribution is $P_{\theta}(\mbf{X}=\mbf{x}, \mbf{Z}=\mbf{z})=\mathcal{N}([\mbf{x};\mbf{z}]; 0, \Sigma)$
is given below

\begin{align}
	\label{eq:pxz}
	P_{\theta}(\mbf{X}=\mbf{x}, \mbf{Z}=\mbf{z})&=P_{\theta}(\mbf{X}=\mbf{x}|\mbf{Z}=\mbf{z})P_{\theta}(\mbf{Z}=\mbf{z})\\
	\label{eq:pxz2}
	&=\mathcal{N}([\mbf{x};\mbf{z}]; 0, \Sigma)\\
 	\Sigma &= \begin{bmatrix}
 		\mbf{W}\mbf{W}^T+\Psi  & \mbf{W} \\
		\mbf{W}^T & I
 	\end{bmatrix}
\end{align}

Using the marginalization property of the multivariate normal, 
we can immediately obtain the marginal probability of $\mbf{X}$:

\begin{align}
	\label{eq:px}
 	P_{\theta}(\mbf{X}=\mbf{x})=\mathcal{N}(\mbf{x}; 0, \mbf{W}\mbf{W}^T+\Psi)
\end{align}

To perform exact inference for $\mbf{Z}$, we also need to know
$P_{\theta}(\mbf{Z}=\mbf{z}|\mbf{X}=\mbf{x})$. This distribution 
is given below.

\begin{align}
	\label{eq:pzgx}
 	P_{\theta}(\mbf{Z}=\mbf{z}|\mbf{X}=\mbf{x})&=\mathcal{N}(\mbf{z}; \mu_{z|x}, \Sigma_{zz|x})\\
	\mu_{z|x} &= \mbf{W}^T(\mbf{W}\mbf{W}^T+\Psi)^{-1}\mbf{x}\\
	\Sigma_{zz|x} &= I - \mbf{W}^T(\mbf{W}\mbf{W}^T+\Psi)^{-1}\mbf{W}
\end{align}


\section{Variational Inference}

While exact computation of $P_{\theta}(\mbf{Z}=\mbf{z}|\mbf{X}=\mbf{x})$ is possible for factoranalysis,
it is sometimes desirable to approximate the posterior using a multi-variate normal distribution
with simpler structure. Let $Q_{\phi}(\mbf{Z}=\mbf{z})=Q_{\phi}(\mbf{Z}=\mbf{z})=\mathcal{N}(\mbf{z};\mbf{m},\mbf{S})$ be such a distribution. The optimal variational parameter values $\mbf{m}$ and $\mbf{S}$ of 
$Q_{\phi}(\mbf{Z}=\mbf{z})$ given the model parameters $\mbf{W}$ and $\Psi$ and the data $\mbf{x}$ 
can be obtained by analytically minimizing the variational inference objective function shown below:

\begin{align}
	\label{eq:KLPQ}
	KL(Q||P)&=E_{Q_{\phi}(\mbf{Z})}[\log Q_{\phi}(\mbf{Z}=\mbf{z}) - \log P_{\theta}(\mbf{Z}=\mbf{z}|\mbf{X}=\mbf{x}) ]
\end{align}

For the purposes of optimization with respect to $\phi$, this equation can be simplified to only depend on $P_{\theta}(\mbf{X}=\mbf{x},\mbf{Z}=\mbf{z})$ since $P_{\theta}(\mbf{X}=\mbf{x})$ is a constant.
This yields:

\begin{align}
	\label{eq:vi}
	J(\phi) &=E_{Q_{\phi}(\mbf{Z})}[\log Q_{\phi}(\mbf{Z}=\mbf{z}) - \log P_{\theta}(\mbf{X}=\mbf{x},\mbf{Z}=\mbf{z}) ]
\end{align}

While the computation of $J(\phi)$ can be carried out analytically for the Factor Analysis model, it is 
aintractable for models with more complex relationships between $\mbf{x}$ and $\mbf{z}$.
In such cases, re-parameterization and stochastic approximation can be used to enable
the computation of variational posteriors. \\

First, we note that if 
$Q_{\phi}(\mbf{Z}=\mbf{z})=\mathcal{N}(\mbf{z};\mbf{m},\mbf{S})$, then drawing
a sample $\mbf{z}\sim Q_{\phi}(\mbf{Z}=\mbf{z})$ is equivalent to drawing a sample
$\epsilon ~ Q'(\epsilon) = \mathcal{N}(\epsilon; 0, I)$, and then transforming that sample deterministically 
as $\mbf{z} = \mbf{m} + \mbf{L}\epsilon$. Here $\epsilon \in \mathbb{R}^K$, and $\mbf{L}$ satisfies 
$\mbf{L}\mbf{L}^T=\mbf{S}$. The matrix $\mbf{L}$ is typically obtained from $\mbf{S}$
using the Cholesky factorization of $\mbf{S}$. Note that if $\mbf{S}$ is diagonal,
then $\mbf{L}$ is also diagonal with elements $\mbf{L}_{kk} = \sqrt{\mbf{S}_{kk}}$.
Using this so-called re-parameterization trick, we can re-write the variational
inference objective as follows:

\begin{align}
	J(\phi)  &=E_{Q_{\phi}(\mbf{Z})}[\log Q_{\phi}(\mbf{Z}=\mbf{z}) 
	   - \log P_{\theta}(\mbf{X}=\mbf{x},\mbf{Z}=\mbf{z}) ] \\
	&=E_{Q_{\phi}(\mbf{Z})}[\log Q_{\phi}(\mbf{Z}=\mbf{z})] 
	  - E_{Q'(\epsilon)}[\log P_{\theta}(\mbf{X}=\mbf{x}, \mbf{Z}=\mbf{m}+\mbf{L}\epsilon)]\\
	\label{eq:SVI}
	&\approx E_{Q_{\phi}(\mbf{Z})}[\log Q_{\phi}(\mbf{Z}=\mbf{z})] 
	    - \frac{1}{T} \sum_{t=1}^T \log P_{\theta}(\mbf{X}=\mbf{x},\mbf{Z}=\mbf{m}+\mbf{L}\epsilon_t)\\
\nonumber 	\epsilon_t&\sim \mathcal{N}(\epsilon_t; 0, I)
\end{align}

Optimizing the final form of the objective function shown above corresponds to stochastic 
variational inference since the integral in the expectation of the second term 
has been replaced by a Monte Carlo average. Since the distribution that we are sampling from
does not depend on the variational parameters being optimized, we can go one step
further and apply black-box stochastic variational inference. \\

This method uses automatic differentiation to compute the gradient of the Monte Carlo  approximation
to the re-parameterized KL objective, and then applies an off-the-shelf optimizer
to find the optimal variational parameters. Again, while these more advanced inference 
techniques are not needed in the case of plain Factor Analysis, they nonetheless 
be applied to the model.


\section{Learning}

To learn the factor analysis model, we need to maximize the log marginal likelihood
function shown below.
%
\begin{align}
\label{eq:logmarglik}
\mathcal{L}(\mathcal{D},\theta) &= \sum_{n=1}^N \log P_{\theta}(\mbf{X}=\mbf{x}_n)\\
&=-\frac{N}{2}\log(|2\pi(\mbf{WW}^T+\Psi)|)-\frac{1}{2}\sum_{n=1}^N \mbf{x}_n^T(\mbf{WW}^T+\Psi)^{-1}\mbf{x}_n
\end{align}

There are several options the maximize this function. First, we can apply direct
gradient-based optimization. To do so, we need to ensure that the constraints
on $\Psi$ are satisfied. We can do so while avoiding the use of constrained
optimization methods by parameterizing $\Psi_{dd}$ as $\exp(\gamma_d)$ where
$\gamma_d\in \mathbb{R}$. With the modification $\theta'=[\mbf{W},\mbf{\gamma}]$, 
we can directly optimize $\mathcal{L}(\mathcal{D},\theta')$
using any off-the-shelf gradient-based optimization method. \\

Second, we can also optimize the model parameters using the EM algorithm. 
The E-step requires the posterior over the latent 
variables $Q_n(\mbf{z}) = P(\mbf{Z}=\mbf{z}|\mbf{X}=\mbf{x}_n)$.
We have:

\begin{align}
	\label{eq:e-step}
 	Q_n(\mbf{z})&=\mathcal{N}(\mbf{z}; \mu_n, \Sigma_n)\\
	\mu_n &= \mbf{W}^T(\mbf{W}\mbf{W}^T+\Psi)^{-1}\mbf{x}_n\\
	\Sigma_n &= I - \mbf{W}^T(\mbf{W}\mbf{W}^T+\Psi)^{-1}\mbf{W}
\end{align}

The M-Step updates are given below:

\begin{align}
	\label{eq:m-step}
	\mbf{W} &\leftarrow \left(\sum_{n=1}^N\mbf{x}_n\mathbb{E}_{Q_n}[\mbf{z}]\right) 
	\left(\sum_{n=1}^N \mathbb{E}_{Q_n}[\mbf{z}\mbf{z}^T]\right)^{-1}\\
	\Psi &= \frac{1}{N}\mbox{diag}\left(\sum_{n=1}^N(\mbf{x}_n - \mbf{W}\mathbb{E}_{Q_n}[\mbf{z}])\mbf{x}_n^T\right)
\end{align}

For models where the marginal likelihood can not be tractably computed,
we can instead use variational learning through maximization of the variational 
free energy in Equation \ref{eq:free1}. When even the variational free energy 
can not be tractably computed, we can approximate the first term using the 
same re-parameterization trick that was introduced for inference. 
This yields the stochastic variational learning objective shown in 
Equation \ref{eq:SVL}.
%
\begin{align}
\label{eq:free1}
\mathcal{F}(\mathcal{D},\theta,\phi_{1:N}) &= \sum_{n=1}^N 
%
E_{Q_{\phi_n}(\mbf{Z})}[
	   \log P_{\theta}(\mbf{X}=\mbf{x}_n,\mbf{Z}=\mbf{z}) 
	  -\log Q_{\phi_n}(\mbf{Z}=\mbf{z}) 
	   ]\\
%
\label{eq:SVL}
&\approx \sum_{n=1}^N \left(\frac{1}{T} \log P_{\theta}(\mbf{X}=\mbf{x}_n,\mbf{Z}=\mbf{m}_n+\mbf{L}_n\epsilon_{tn})
-E_{Q_{\phi_n}(\mbf{Z})}[\log Q_{\phi_n}(\mbf{Z}=\mbf{z})]\right)\\
\nonumber \epsilon_{tn} &\sim \mathcal{N}(\epsilon_{tn}; 0, I)
\end{align}








\end{document} 